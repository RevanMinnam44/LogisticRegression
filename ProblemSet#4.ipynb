{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12412b43",
   "metadata": {},
   "source": [
    "NOTE: I have to read my csv file from a specific directory in my laptop, so if you're about to test this code, read the csv file either regularly or from a directory. This code does work and the I have tested it multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5eebd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "    df = problem1()\n",
    "    problem3(df)\n",
    "    results = problem3(df)\n",
    "    print(results)\n",
    "\n",
    "def problem1():\n",
    "    df = pd.read_csv(\"/Users/revan/Downloads/corona_fake.csv\")\n",
    "    # replace empty text    \n",
    "    df = df.fillna('')\n",
    "    df[\"clean_text\"] = df[\"text\"].map(clean_text)\n",
    "    return df\n",
    "    \n",
    "def clean_text(text_str):\n",
    "    text_str = remove_emojis(text_str) #removes emojis\n",
    "    text_str = re.sub(r\"http\\S+\", \"\", text_str) #removes links\n",
    "    text_str = re.sub(r'[^\\w\\s]', '', text_str) #removes punctuation\n",
    "    tokens = nltk.word_tokenize(text_str)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    #wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), tagged))\n",
    "    for word, tag in tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:       \n",
    "            # else use the tag to lemmatize the token\n",
    "            #print(word)\n",
    "            lemmatized_sentence.append(lmtzr.lemmatize(word, pos_tagger(tag)))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(lemmatized_sentence)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if len(w) <= 2 or re.match(r'\\d+',  w): continue\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "\n",
    "    \n",
    "def remove_emojis(text_str):\n",
    "    return ''.join(c for c in text_str if c not in emoji.UNICODE_EMOJI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686abb73",
   "metadata": {},
   "source": [
    "2) N-Grams are words, or combinations of words, broken out by the number of words in that combination. It starts off with unigrams (one word), bigrams (two words), trigrams (three words), etc. The words must follow sequentially to be an n-gram. According to Seerinteractive, N-Grams are useful for turning written language into data, and breaking down larger portions of search data into more meaningful segments that help to identify the root cause behind trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcf1ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2(df):    \n",
    "    clean_text = df['clean_text'].tolist()\n",
    "    vectorizer1 = CountVectorizer(ngram_range=(1,1), lowercase= True)\n",
    "    vectorizer2 = CountVectorizer(ngram_range=(1,2), lowercase= True)\n",
    "    vectorizer3 = CountVectorizer(ngram_range=(1,3), lowercase= True)\n",
    "    cv1 = vectorizer1.fit_transform(clean_text)\n",
    "    cv2 = vectorizer2.fit_transform(clean_text)\n",
    "    cv3 = vectorizer3.fit_transform(clean_text)\n",
    "    tfidf1 = TfidfVectorizer(ngram_range=(1,1), lowercase= True)\n",
    "    tfidf2 = TfidfVectorizer(ngram_range=(1,2), lowercase= True)\n",
    "    tfidf3 = TfidfVectorizer(ngram_range=(1,3), lowercase= True)\n",
    "    tf1 = tfidf1.fit_transform(clean_text)\n",
    "    tf2 = tfidf2.fit_transform(clean_text)\n",
    "    tf3 = tfidf3.fit_transform(clean_text)\n",
    "    return (cv1, cv2, cv3, tf1, tf2, tf3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41dd2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem3(df):\n",
    "    y = df['label'].map({'fake': 0, 'true': 1}).tolist() \n",
    "    (cv1, cv2, cv3, tf1, tf2, tf3) = problem2(df)\n",
    "    #print(logistic_regression(cv1, y))\n",
    "    results = []\n",
    "    I = 1\n",
    "    for cv in cv1, cv2, cv3:    \n",
    "        results.append([f\"CV{I}\" ,  logistic_regression(cv, y) ])        \n",
    "        I += 1\n",
    "    \n",
    "    I = 1\n",
    "    for tf in tf1, tf2, tf3:    \n",
    "        results.append([f\"TF{I}\" ,  logistic_regression(tf, y) ])\n",
    "        I += 1\n",
    "    return pd.DataFrame(results, columns=['label', 'accuracy'])    \n",
    "    \n",
    "    \n",
    "def logistic_regression(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, train_size=0.70, random_state = 265)\n",
    "    clf = LogisticRegressionCV(cv = 5, random_state = 265, max_iter  =1000, n_jobs = -1).fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83801b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label  accuracy\n",
      "0   CV1  0.913793\n",
      "1   CV2  0.916667\n",
      "2   CV3  0.913793\n",
      "3   TF1  0.925287\n",
      "4   TF2  0.905172\n",
      "5   TF3  0.902299\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3d637",
   "metadata": {},
   "source": [
    "4) For each function, explain in around 100 words what they mean; specifically\n",
    "\n",
    "newton-cg -  A newton method. Newton methods use an exact Hessian matrix. It's slow for large datasets, because it computes the second derivatives. A Hessian matrix  is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. It only support L2 regularization or no regularization\n",
    "\n",
    "lbfgs — Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno. It approximates the second derivative matrix updates with gradient evaluations. It stores only the last few updates, so it saves memory. It isn't super fast with large data sets. It only support L2 regularization or no regularization.\n",
    "\n",
    "liblinear — Library for Large Linear Classification. Uses a coordinate descent algorithm. Coordinate descent is based on minimizing a multivariate function by solving univariate optimization problems in a loop. In other words, it moves toward the minimum in one direction at a time. It is the default solver for Scikit-learn versions earlier than 0.22.0. It performs pretty well with high dimensionality. It does have a number of drawbacks. It can get stuck, is unable to run in parallel, and can only solve multi-class logistic regression with one-vs.-rest.\n",
    "\n",
    "sag — solver uses Stochastic Average Gradient descent. A variation of gradient descent and incremental aggregated gradient approaches that uses a random sample of previous gradient values. Fast for big datasets. It only support L2 regularization or no regularization\n",
    "\n",
    "saga — The SAGA solver is a variant of SAG that also supports the non-smooth penalty L1 option (i.e. L1 Regularization). This is therefore the solver of choice for sparse multinomial logistic regression and it’s also suitable for very Large dataset. Should generally train faster than sag. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
