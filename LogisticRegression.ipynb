{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2bb091",
   "metadata": {},
   "source": [
    "1) Read the following highly-cited article by Fearon and Laitin (2003): https://cisac.fsi.stanford.edu/publications/ethnicity_insurgency_and_civil_war  \n",
    "\n",
    "a. During a certain period of time, there was a lot of violence, resulting in the deaths of millions. A huge contributor to the death toll were civil wars. Specifically, 127 civil wars that killed at\n",
    "least 1,000, 25 of which were ongoing in 1999. As a result of these conflicts, an estimated total of 16.2 million died.\n",
    "\n",
    "This paper explores many questions surrounding those civil wars: \n",
    "\n",
    "What explains the recent prevalence of violent civil conflict around the world? \n",
    "Is it due to the end of the Cold War and associated changes in the international system, or is it the result of longer-term trends? \n",
    "Why have some countries had civil wars while others have not?   \n",
    "\n",
    "This paper answers/tests these questions to find a definitve conclusion. They will utilize the data from that period. However, the authors clarified that the causes of civil war in the 90s was not due to the end of the Cold War and associated changes in the international system, religiously or ethnically diversed countries are not more prone to civil war, and that they cannot predict a start of a civil war"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbaf4eb",
   "metadata": {},
   "source": [
    "b. There are 127 observations because the data recorded 127 conflicts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68116753",
   "metadata": {},
   "source": [
    "c. Independent: Prior War, Per Capita Income, Log(population), Log(%Maintanous), Noncontiguous state, Oil exporter, New state, Instability, Democracy, Ethnic fractionalization   \n",
    "\n",
    "Religious/Ethnic Diversity, Political Democracy/Issues, Income Ineqaulity, \n",
    "\n",
    "Dependent: Probability of: Civil War, \"Ethnic\" War, Civil War (Dummy Variable), Civil War (Plus Empires), Civil War (COW) \n",
    "\n",
    "(COW) = Correlates of War"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb332a",
   "metadata": {},
   "source": [
    "d. The coefficents (the values that are not in parentheses) represent the relatioship between the independent and dependent varaibles. If the coefficent is positive, then the relationship is positive. If the coefficent is negative, then the relationship is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25f9dc",
   "metadata": {},
   "source": [
    "e. The following independent variables have a positive relationship with each dependent variable: log(population), log(% mountainous), Oil Exporter, New State, Instability, Democracy, Religious fractionalization, Anocracy \n",
    "\n",
    "The following independent variables have a positive relationship with every dependent variable except for Civil War (COW). They have a negative relationship with Civil War (COW): Noncontigous state, Ethnic Fractionalization\n",
    "\n",
    "The following independent variables have a negative relationship with each dependent variable: Prior War, Per Capita Income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1d4f8",
   "metadata": {},
   "source": [
    "f. Overall, the independent variable that has a greater range is the New stae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8053d",
   "metadata": {},
   "source": [
    "2) Build a two-class logistic regression model from scratch. Load the Breast Cancer Wisconsin Dataset provided by sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generateXVector(x):\n",
    "    vectorX = np.c_[np.ones((len(x), 1)), x]\n",
    "    return vectorX\n",
    "\n",
    "def theta_init(x):\n",
    "    theta = np.random.randn(len(x[0])+1, 1)\n",
    "    return theta\n",
    "\n",
    "def sigmoid_f(x):\n",
    "    return 1/(1+np.e**(-x))\n",
    "\n",
    "def classifier_f(x, theta):\n",
    "    h_theta = sigmoid_f(x.dot(theta))\n",
    "    return h_theta\n",
    "\n",
    "def gradient_f(x,y,theta):\n",
    "    m = len(x)\n",
    "    grad = 1/m * x.T.dot(sigmoid_f(x.dot(theta))-y)\n",
    "    return grad\n",
    "\n",
    "def binary_loss_f(y,y_pred):\n",
    "    return np.dot(y.T, np.log(y_pred))\n",
    "\n",
    "def logistic_regression(x, y, learningrate, iterations):\n",
    "    y_new = np.reshape(y, (len(y), 1))\n",
    "    cost_1st = []\n",
    "    vectorX = generateXVector(x)\n",
    "    theta = theta_init(x)\n",
    "    for i in range(iterations):\n",
    "        theta = theta - learningrate * gradient_f(vectorX, y_new, theta)\n",
    "        y_pred = classifier_f (vectorX, theta)\n",
    "        entropy_1 = binary_loss_f(y_new ,y_pred)\n",
    "        entropy_2 = binary_loss_f(1-y_new,1-y_pred)\n",
    "        cost_value = -np.sum(entropy_1 + entropy_2) / len(y_pred)\n",
    "    return theta, cost_value\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "Y = data.target\n",
    "y_1 = pd.DataFrame(Y)\n",
    "scalar = MinMaxScaler()\n",
    "x = scalar.fit_transform(X)\n",
    "y = scalar.fit_transform(y_1)\n",
    "result = logistic_regression(x, y, 0.01, 10000)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b170c",
   "metadata": {},
   "source": [
    "3) Implement the three following cross-validation algorithms from scratch: a. Leave-one-out cross-validation b. K-fold cross-validation c. Train-test split cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc54e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import sklearn.datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "random.seed(265)\n",
    "\n",
    "\n",
    "def validate_leave_one_out(X, Y):\n",
    "    lm = LinearRegression()\n",
    "    ix_data  = list(range(X.shape[0]))\n",
    "    X_index = np.array(ix_data)\n",
    "\n",
    "    mse_list = []\n",
    "    for i in ix_data:\n",
    "        train_ix = np.delete(ix_data,i)\n",
    "        test_ix = np.array([i])\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        Y_train, Y_test = Y[train_ix], Y[test_ix]\n",
    "        lm.fit(X_train, Y_train)\n",
    "        Y_predict = lm.predict(X_test)\n",
    "        mse = mean_squared_error(Y_test, Y_predict)\n",
    "        mse_list.append(mse)\n",
    "    print(\"LOOCV: {}\".format(np.mean(mse_list)))\n",
    "    return np.mean(mse_list)\n",
    "\n",
    "# K fold\n",
    "def k_fold_validation(X, Y, K=5):\n",
    "    lm = LinearRegression()\n",
    "    N = X.shape[0]\n",
    "    X_K = np.array_split(X, 5)\n",
    "    Y_K = np.array_split(Y, 5)\n",
    "    mse_list = []\n",
    "    for i in range(K):\n",
    "        X_test = X_K[i]\n",
    "        Y_test = Y_K[i]\n",
    "        X_train = np.concatenate(X_K[:i] + X_K[i+1:])\n",
    "        Y_train = np.concatenate(Y_K[:i] + Y_K[i+1:])\n",
    "        lm.fit(X_train, Y_train)\n",
    "        Y_predict = lm.predict(X_test)\n",
    "        mse = mean_squared_error(Y_test, Y_predict)\n",
    "        mse_list.append(mse)\n",
    "    print(\"K-fold mse: {}\".format(np.mean(mse_list)))\n",
    "    return np.mean(mse_list)\n",
    "\n",
    "\n",
    "def train_test_split_validation(X, Y, test_size=0.3, train_size=0.70):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, train_size=train_size, random_state=265)\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train, Y_train)\n",
    "    Y_predict = lm.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, Y_predict)\n",
    "    print(\"train_test_split: {}\".format(mse))\n",
    "\n",
    "def test():\n",
    "    df = sklearn.datasets.fetch_california_housing(download_if_missing=True, return_X_y=False,\n",
    "                                              as_frame=True)\n",
    "    sc = MinMaxScaler()\n",
    "    X = df.data.values.copy()\n",
    "    X_scaled = sc.fit_transform(X)\n",
    "    y = df.target.values\n",
    "    # split the dataset\n",
    "    Y_scaled = sc.fit_transform(y.reshape((y.shape[0], 1)))\n",
    "    Y_scaled = Y_scaled.reshape((Y_scaled.shape[0],))\n",
    "    k_fold_validation(X_scaled, Y_scaled, K=5)\n",
    "    train_test_split_validation(X_scaled, Y_scaled, test_size=0.30, train_size=0.70)\n",
    "    validate_leave_one_out(X_scaled, Y_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ca7826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold mse: 0.023734108506413422\n",
      "train_test_split: 0.023161942880990504\n",
      "LOOCV: 0.02245687523556031\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a9c48",
   "metadata": {},
   "source": [
    "Based on the result, LOOCV < Train/Test Split < K-Fold. LOOCV is the smallest because it uses all, checking it one-by-one. However, while it retrieves the smalles MSE, it takes the longest because the dataset is large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
